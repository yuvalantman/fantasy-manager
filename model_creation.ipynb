{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce8335c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_fantasy_data(df):\n",
    "    # Step 1: Create HOME_GAME from MATCHUP before dropping it\n",
    "    df[\"HOME_GAME\"] = df[\"MATCHUP\"].apply(lambda x: 1 if \"vs.\" in x else 0)\n",
    "\n",
    "    # Step 2: Drop unneeded columns\n",
    "    drop_cols = [\n",
    "        \"Unnamed: 0\", \"W\", \"L\", \"MIN\", \"FORM\",\n",
    "        \"TEAM_ID\", \"GAME_ID\", \"PLAYER_ID\", \"OPPONENT_ID\", \"MATCHUP\", \"OPPONENT_ABBR\"\n",
    "    ]\n",
    "    df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    # Step 3: Convert HEIGHT from string to inches\n",
    "    def height_to_inches(h):\n",
    "        try:\n",
    "            feet, inches = map(int, h.split('-'))\n",
    "            return feet * 12 + inches\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    df[\"HEIGHT_IN\"] = df[\"HEIGHT\"].apply(height_to_inches)\n",
    "\n",
    "    # Step 4: Encode POSITION as categorical code\n",
    "    df[\"POSITION\"] = df[\"POSITION\"].astype(\"category\").cat.codes\n",
    "\n",
    "    # Step 5: Drop original HEIGHT column\n",
    "    df = df.drop(columns=[\"HEIGHT\"], errors='ignore')\n",
    "\n",
    "    # Step 6: Drop rows with any remaining missing values (if any)\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6227bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/all_players_stats.csv\")\n",
    "df_future = pd.read_csv(\"data/all_players_stats2025.csv\")\n",
    "\n",
    "# Extract all GAME_IDs from the 2024–25 dataset\n",
    "future_game_ids = df_future[\"GAME_ID\"].unique()\n",
    "\n",
    "# Remove rows from the main dataset where GAME_ID is in the future set\n",
    "df_main_filtered = df[~df[\"GAME_ID\"].isin(future_game_ids)].copy()\n",
    "cleaned_df = preprocess_fantasy_data(df_main_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeeefa77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['G', 'G-F', 'F', 'C-F', 'F-G', 'C', 'F-C'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_position = df[\"POSITION\"].unique()\n",
    "df_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed012e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = cleaned_df.drop(columns=[\"NBA_FANTASY_PTS\"])\n",
    "y = cleaned_df[\"NBA_FANTASY_PTS\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b67dc6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 7.617190276411412\n",
      "R²: 0.5332614313798828\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_val)\n",
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_val, y_pred))\n",
    "print(\"R²:\", r2_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e7e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "MAE: 7.64483353818653\n",
      "R²: 0.5287554237347603\n",
      "Best Parameters: {'bootstrap': False, 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 138}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define hyperparameter space\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(50, 150),\n",
    "    \"max_depth\": [None] + list(range(10, 51, 10)),\n",
    "    \"max_features\": [\"sqrt\", \"log2\", 0.5, 0.8],\n",
    "    \"min_samples_split\": randint(2, 10),\n",
    "    \"min_samples_leaf\": randint(1, 5),\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "# Use a small subset for tuning\n",
    "# X_sample = X_train.sample(n=25000, random_state=42)\n",
    "X_sample = X_train.sample(n=10000, random_state=42)\n",
    "\n",
    "y_sample = y_train.loc[X_sample.index]\n",
    "\n",
    "# Set up search\n",
    "rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=rf_base,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=20,\n",
    "#     cv=3,\n",
    "#     scoring=\"neg_mean_absolute_error\",\n",
    "#     verbose=2,\n",
    "#     n_jobs=-1,\n",
    "#     random_state=42\n",
    "# )\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,              # ⏩ Try just 10 combinations\n",
    "    cv=2,                   # ⏩ 2-fold CV instead of 3\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_sample, y_sample)\n",
    "\n",
    "# Evaluate on validation set\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_val)\n",
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_val, y_pred))\n",
    "print(\"R²:\", r2_score(y_val, y_pred))\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a8e688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Obtaining dependency information for lightgbm from https://files.pythonhosted.org/packages/5e/23/f8b28ca248bb629b9e08f877dd2965d1994e1674a03d67cd10c5246da248/lightgbm-4.6.0-py3-none-win_amd64.whl.metadata\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\yuval\\anaconda3\\lib\\site-packages (from lightgbm) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\yuval\\anaconda3\\lib\\site-packages (from lightgbm) (1.11.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.5 MB 393.8 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.1/1.5 MB 651.6 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 893.0 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.5 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.5/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.7/1.5 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.2/1.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99d044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23282148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2870\n",
      "[LightGBM] [Info] Number of data points in the train set: 252084, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 21.153602\n",
      "MAE: 7.505963643119989\n",
      "R²: 0.5448201404505459\n"
     ]
    }
   ],
   "source": [
    "X = cleaned_df.drop(columns=[\"NBA_FANTASY_PTS\"])\n",
    "y = cleaned_df[\"NBA_FANTASY_PTS\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred = lgb_model.predict(X_val)\n",
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_val, y_pred))\n",
    "print(\"R²:\", r2_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ff540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 7.5342202640075655\n",
      "R²: 0.5409208340855014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Scale + generate polynomial features (degree 2)\n",
    "poly_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PolynomialFeatures(degree=2, include_bias=False),\n",
    "    Ridge(alpha=10, random_state=42)\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "poly_model.fit(X_train, y_train)\n",
    "y_pred_poly = poly_model.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "print(\"MAE:\", mean_absolute_error(y_val, y_pred_poly))\n",
    "print(\"R²:\", r2_score(y_val, y_pred_poly))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ec10256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2871\n",
      "[LightGBM] [Info] Number of data points in the train set: 252084, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -0.902078\n",
      "[LightGBM] [Info] Start training from score -0.984321\n",
      "[LightGBM] [Info] Start training from score -1.511489\n",
      "[[17416  5987  2167]\n",
      " [11718 10335  1498]\n",
      " [ 8977  2338  2586]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    expected       0.46      0.68      0.55     25570\n",
      "        over       0.55      0.44      0.49     23551\n",
      "       under       0.41      0.19      0.26     13901\n",
      "\n",
      "    accuracy                           0.48     63022\n",
      "   macro avg       0.47      0.44      0.43     63022\n",
      "weighted avg       0.48      0.48      0.46     63022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def classify_performance(row):\n",
    "    if row[\"NBA_FANTASY_PTS\"] > row[\"LAST_30_GAMES_AVG_FORM\"] + 5:\n",
    "        return \"over\"\n",
    "    elif row[\"NBA_FANTASY_PTS\"] < row[\"LAST_30_GAMES_AVG_FORM\"] - 5:\n",
    "        return \"under\"\n",
    "    else:\n",
    "        return \"expected\"\n",
    "\n",
    "cleaned_df[\"PERFORMANCE_CLASS\"] = cleaned_df.apply(classify_performance, axis=1)\n",
    "\n",
    "# Prepare features and target\n",
    "X = cleaned_df.drop(columns=[\"NBA_FANTASY_PTS\", \"PERFORMANCE_CLASS\"])\n",
    "y = cleaned_df[\"PERFORMANCE_CLASS\"]\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "clf = LGBMClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_val)\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2700ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2858\n",
      "[LightGBM] [Info] Number of data points in the train set: 15000, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -0.899106\n",
      "[LightGBM] [Info] Start training from score -0.974450\n",
      "[LightGBM] [Info] Start training from score -1.534021\n",
      "Best Params: {'num_leaves': 20, 'n_estimators': 300, 'max_depth': 15, 'learning_rate': 0.05, 'class_weight': None}\n",
      "[[16339  6711  2520]\n",
      " [11315 10470  1766]\n",
      " [ 8511  2857  2533]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    expected       0.45      0.64      0.53     25570\n",
      "        over       0.52      0.44      0.48     23551\n",
      "       under       0.37      0.18      0.24     13901\n",
      "\n",
      "    accuracy                           0.47     63022\n",
      "   macro avg       0.45      0.42      0.42     63022\n",
      "weighted avg       0.46      0.47      0.45     63022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Reduce to a 15k sample for faster search\n",
    "X_sample = X_train.sample(n=15000, random_state=42)\n",
    "y_sample = y_train.loc[X_sample.index]\n",
    "\n",
    "# Hyperparameter space\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [6, 10, 15],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"num_leaves\": [20, 40, 60],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "# Model\n",
    "lgb_base = LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Randomized search\n",
    "search = RandomizedSearchCV(\n",
    "    lgb_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15,\n",
    "    scoring='f1_weighted',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "search.fit(X_sample, y_sample)\n",
    "\n",
    "# Use best model\n",
    "best_lgb = search.best_estimator_\n",
    "y_pred = best_lgb.predict(X_val)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Best Params:\", search.best_params_)\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2c814f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25334   235     1]\n",
      " [22062  1489     0]\n",
      " [13879    16     6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    expected       0.41      0.99      0.58     25570\n",
      "        over       0.86      0.06      0.12     23551\n",
      "       under       0.86      0.00      0.00     13901\n",
      "\n",
      "    accuracy                           0.43     63022\n",
      "   macro avg       0.71      0.35      0.23     63022\n",
      "weighted avg       0.68      0.43      0.28     63022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Predict probabilities\n",
    "probs = best_lgb.predict_proba(X_val)\n",
    "classes = best_lgb.classes_\n",
    "\n",
    "# Threshold-based prediction logic\n",
    "threshold = 0.8\n",
    "y_pred_thresh = []\n",
    "\n",
    "for p in probs:\n",
    "    prob_dict = dict(zip(classes, p))\n",
    "    if prob_dict[\"over\"] >= threshold:\n",
    "        y_pred_thresh.append(\"over\")\n",
    "    elif prob_dict[\"under\"] >= threshold:\n",
    "        y_pred_thresh.append(\"under\")\n",
    "    else:\n",
    "        y_pred_thresh.append(\"expected\")\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_val, y_pred_thresh))\n",
    "print(classification_report(y_val, y_pred_thresh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e81e731b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold = 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    expected       0.43      0.85      0.57     25570\n",
      "        over       0.63      0.30      0.40     23551\n",
      "       under       0.45      0.04      0.07     13901\n",
      "\n",
      "    accuracy                           0.47     63022\n",
      "   macro avg       0.50      0.40      0.35     63022\n",
      "weighted avg       0.51      0.47      0.40     63022\n",
      "\n",
      "\n",
      "Threshold = 0.6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    expected       0.42      0.93      0.58     25570\n",
      "        over       0.70      0.20      0.31     23551\n",
      "       under       0.53      0.01      0.02     13901\n",
      "\n",
      "    accuracy                           0.45     63022\n",
      "   macro avg       0.55      0.38      0.30     63022\n",
      "weighted avg       0.55      0.45      0.36     63022\n",
      "\n",
      "\n",
      "Threshold = 0.7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    expected       0.42      0.97      0.59     25570\n",
      "        over       0.78      0.12      0.21     23551\n",
      "       under       0.53      0.00      0.00     13901\n",
      "\n",
      "    accuracy                           0.44     63022\n",
      "   macro avg       0.58      0.37      0.27     63022\n",
      "weighted avg       0.58      0.44      0.32     63022\n",
      "\n",
      "\n",
      "Threshold = 0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    expected       0.41      0.99      0.58     25570\n",
      "        over       0.86      0.06      0.12     23551\n",
      "       under       0.86      0.00      0.00     13901\n",
      "\n",
      "    accuracy                           0.43     63022\n",
      "   macro avg       0.71      0.35      0.23     63022\n",
      "weighted avg       0.68      0.43      0.28     63022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.5, 0.6, 0.7, 0.8]\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = []\n",
    "    for p in probs:\n",
    "        prob_dict = dict(zip(classes, p))\n",
    "        if prob_dict[\"over\"] >= thresh:\n",
    "            y_pred_thresh.append(\"over\")\n",
    "        elif prob_dict[\"under\"] >= thresh:\n",
    "            y_pred_thresh.append(\"under\")\n",
    "        else:\n",
    "            y_pred_thresh.append(\"expected\")\n",
    "    \n",
    "    print(f\"\\nThreshold = {thresh}\")\n",
    "    print(classification_report(y_val, y_pred_thresh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e04e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
